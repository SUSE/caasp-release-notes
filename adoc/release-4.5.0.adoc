== What Is New in 4.5.0

=== Migrating to 4.5.0

If you are coming from the latest release, just follow the instructions described link:{docurl}single-html/caasp-admin/#caasp-migrate-4.5[here].
Otherwise, if you are coming from older releases, you will have to upgrade to the previous {productname} release first, and then follow the link above.

=== Base Operating System Is Now {slsa} 15 SP2

{productname} 4.5 uses standard *{slsa} 15 SP2* as the base platform OS.
{productname} can be installed as an extension on top of that. Because {slsa} 15 is
designed to address both cloud-native and legacy workloads.
these changes make it easier for customers who want to modernize their
infrastructure by moving existing workloads to a {kube} framework.

==== Changes in the Installation Media

Please pay attention to the change in the installation media of {slsa} 15 SP2. The Unified Installer and Packages DVDs known from SUSE {slsa} 15 SP1 are deprecated and have been replaced by Online Installation Media and Full Installation Media, for further details see link:https://www.suse.com/releasenotes/x86_64/SUSE-SLES/15-SP2/#_changes_in_15sp2[this section] of the *{slsa} 15 SP2 Release Notes*.

For further information on notable changes when going from {slsa} 15 SP1 to {slsa} 15 SP2, also refer to the link:https://www.suse.com/releasenotes/x86_64/SUSE-SLES/15-SP2/[{slsa} 15 SP2 Release Notes].

=== Support disabling routable check in {vmware}

{vmware} {tf} config now supports the option `wait_for_guest_net_routable` to disable {tf} from waiting for a routable network on VM creation. This is useful to prevent an upstream issue in which {tf} hangs waiting for a routable IP even when the VM has one. For more information check the link:https://github.com/hashicorp/terraform-provider-vsphere/issues/1127[bug report]

=== Changes to the {kube} Stack

==== Updated Kubernetes

{productname} 4.5.0 comes with *{kube} 1.18*. The new {kube} version includes for example notable upgrades to kubectl, storage enhancements, Horizontal Pod Autoscaler (HPI) and advanced scheduling. Some of these features have been available in {productname} for a couple of versions now.

You can find a list of changes directly relevant to {productname} here: link:{docurl}single-html/caasp-admin/#k8s-changes-117-118[Changes from {kube} 1.17 to 1.18]

For a more generalized summary, you can view the link:https://www.suse.com/c/whats-new-in-kubernetes-v1-18-0/[SUSE blog] or see the full version in the link:https://kubernetes.io/docs/setup/release/notes/[official Kubernetes documentation].

==== New Kubernetes package names

From {productname} version 4.5.0 onwards, {kube} packages are built with new package names; for example:

* `kubernetes-1.18-1.18.6-1.1.src.rpm`
* `kubernetes-1.18-apiserver-1.18.6-1.1.x86_64.rpm`
* `kubernetes-1.18-client-1.18.6-1.0.x86_64.rpm`
* `kubernetes-1.18-controller-manager-1.18.6-1.1.x86_64.rpm`
* `kubernetes-1.18-extra-1.18.6-1.1.x86_64.rpm`
* `kubernetes-1.18-kubeadm-1.18.6-1.1.x86_64.rpm`
* `kubernetes-1.18-kubelet-1.18.6-1.1.x86_64.rpm`
* `kubernetes-1.18-proxy-1.18.6-1.1.x86_64.rpm`
* `kubernetes-1.18-scheduler-1.18.6-1.1.x86_64.rpm`

In {productname} {kube} packages use the `kubernetes-1.18-name` pattern. The reason for this is because the link:https://kubic.opensuse.org/[openSUSE Kubic] project already uses the pattern `kubernetes1.18-name` but with different package (dependency) semantics underneath.

Having two identically named packages from SUSE (one from {productname}, one from  Kubic) with different semantics could lead to confusion and conflicts/problems with installation of the software from either project.

[#crio-118-config-update]
==== CRI-O

This release upgrades `{crio}` to 1.18, which brings support for {kube} 1.18.
This new version, though, requires some manual steps for the upgrade, since the configuration files have been migrated from `sysconfig` to `/etc/crio/crio.conf.d/`.
In order to upgrade, then, you will have to perform the following command before upgrading each node:

----
skuba cluster upgrade localconfig
----

After this you will be able to perform all the `skuba` commands that you would call normally when upgrading {productname}.

=== Kubernetes Audit Log with rsyslog agent

Starting with this version of {productname}, the {kube} audit log will be forwarded to the centralized logging service using `log-agent-rsyslog`.

=== Helm 3

Helm 3 is now available in the {productname} zypper repositories.
Helm 3 includes many enhancements over Helm 2.

* Removal of Tiller, which simplifies deployment and management.
* Configuration is now stored in {kube} Secrets rather than ConfigMaps.
* Chart releases are now scoped to {kube} namespaces.
* Charts can be searched for in Helm Hub or repositories using the `helm search` command.
* The chart `apiVersion` has been bumped to "v2" to allow specification changes.
* Several `helm` command line options have been changed, notably the name for a release is now a required parameter.

These changes and more are documented by the community in the link:https://helm.sh/docs/faq/#changes-since-helm-2[Helm FAQs] and link:https://v3.helm.sh/docs/topics/v2_v3_migration/[Migrating 2 to 3 document].

Helm 2 is still installed by default to avoid forcing a migration of chart releases when upgrading {productname}.
The Helm 3 version is not a drop in upgrade of Helm 2, though most charts written for "apiVersion: v1" will work with Helm 3.
However, as the stated end of community support for Helm 2 is November 13, 2020, *you should migrate to Helm 3 as soon as reasonable*.

*Please Note:* We recommend using Helm 3 for fresh installations. See link:https://helm.sh/blog/covid-19-extending-helm-v2-bug-fixes/[Official Helm blog].

The Helm 2 and Helm 3 packages have been named `helm2` and `helm3` to allow both versions to be installed to facilitate migration.
Through the use of `update-alternatives` the version used by 'helm' command line can be set.
For detailed steps to select alternatives, see link:{docurl}single-html/caasp-admin/#helm_tiller_install[the Administration Guide].

In the next {productname} release, the default Helm version will be switched to Helm 3, and in the following release Helm 2 will be dropped.

=== Required Actions

==== Update CRI-O configuration format

You must update the {crio} configuration files on each node before upgrading, please refer to <<crio-118-config-update>>.

==== (VMWare) Enable "Jumbo Frames"

To avoid running into problems with certain integrations and components, VMWare users must enable "Jumbo Frames" and configure maximum transmission unit (MTU) to 9000.
Refer to: https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.networking.doc/GUID-53F968D9-2F91-41DA-B7B2-48394D997F2A.html

== Documentation Changes

* Added the link:{docurl}caasp-admin/#caasp-migration[migration instructions from {productname} 4.0.x/4.2.x]
* Split the air gap deployment guide into a separate document: link:{docurl}single-html/caasp-airgap/[Airgap Deployment Guide]
* Installation steps for Helm 3 and alternate command lines for Helm 3 documented in link:{docurl}single-html/caasp-admin/#helm_tiller_install[the Administration Guide].
* Instructions on how to migrate from Helm 2 to 3, see link:{docurl}single-html/caasp-admin/#helm-2to3-migration[the Administration Guide].
* New chapter on link:{docurl}single-html/caasp-admin/#addon-certificate-rotation[Addon Certificate Rotation in the Administration Guide].
* New chapter on link:{docurl}single-html/caasp-admin/#_gpu_dependent_workloads[GPU-Dependent Workloads in the Administration Guide].
* link:{docurl}single-html/caasp-admin/#_logging[Completely overhauled logging section], reordered Admin guide to incorporate this change.
* Various other fixes and improvements, refer to: https://github.com/SUSE/doc-caasp/releases/tag/release-4.5


== Known Issues

=== In the upgrade process, after the restart of CRI-O and kubelet, some pods might not run properly

This can happen when there are multiple instances of a PodSandbox in a "NotReady" state. As a workaround please make sure to remove any pod in the "NotReady" state using crictl rmp <podid>. Further it is advisable to drain the node that is being upgrade before actually starting the upgrade procedure.

The upstream fix is https://github.com/cri-o/cri-o/pull/4006 which will be included in the next release.

Reference: https://github.com/SUSE/avant-garde/issues/1808


=== etcd: CVE-2020-15106 and CVE-2020-15112

Note the version of etcd shipped with CaaSP 4.5.0 contains two security issues identified as CVE-2020-15106 and CVE-2020-15112

The etcd endpoints should only be accessible inside the cluster if you have set up the firewall rules / network segmentation, following our suggestions in the admin guide; etcd should only be accessible by k8s nodes (or by trusted nodes). Exploiting this vulnerability requires an attacker to take control of the etcd leader in order to send crafted WAL entries, which means access to the SSL certs or local machine access.

Fixes for these will be provided as a maintenance update.


=== envoy: CVE-2020-12605,CVE-2020-8663,CVE-2020-12603 and CVE-2020-12604

Note that the version of envoy shipped with CaaSP 4.5.0 contains security issues idendified as CVE-2020-12605,CVE-2020-8663,CVE-2020-12603 and CVE-2020-12604

These are "Denial of Service" vulnerabilities, and do not expose systems to unauthorized access or data exfiltration. A fix for them will be provided as a maintenance update.


=== Helm 2to3 migration plugin requires internet connection to install

The installer for the Helm 2to3 plugin is written to pull the plugin from the official community github site at link:https://github.com/helm/helm-2to3[github.com/helm/helm-2to3].
This could cause a problem in an air-gapped {productname} installation where an open internet connection is not available.

The simplest workaround is to move the management system (such as a laptop) out of the internal network, install the plugin, then move back in to perform the migration.


===  Upgraded v4.5 cluster is running etcd from v4 namespace (bsc#1176225)

After performing upgrade from CaaSP 4.2.3 to CaaSP 4.5.0 GMC3 the migrated cluster is running etcd from v4 namespace then. If 4.5.0 is deployed from scratch the cluster is using correct image from v4.5 namespace.

The outdated etcd image from v4 namespace doesn't have any impact for the cluster functionality.
